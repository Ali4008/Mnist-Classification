{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9uSB87DCwYDt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3J4hdBL5M-i",
        "outputId": "a59d6460-df80-4b00-bbfe-815aeea88384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST data\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "# Normalize the data\n",
        "X /= 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XQZM9rp5P4U",
        "outputId": "99aecd9e-3005-4536-da55-b43375cc65ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.to_numpy().reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.to_numpy().reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W42OzsPc5Wsp"
      },
      "outputs": [],
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
        "\n",
        "# Derivative of ReLU\n",
        "def relu_derivative(Z):\n",
        "    return Z > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "we1Xdb-26DVq"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(input_size, hidden_size, output_size):\n",
        "    np.random.seed(42)\n",
        "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
        "    b1 = np.zeros((hidden_size, 1))\n",
        "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
        "    b2 = np.zeros((output_size, 1))\n",
        "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Sobv-0VF6FYh"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
        "    Z1 = np.dot(W1, X.T) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return A2, {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6xh_ioka6HSE"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(X, Y, cache, parameters):\n",
        "    m = X.shape[0]\n",
        "    Z1, A1, Z2, A2 = cache[\"Z1\"], cache[\"A1\"], cache[\"Z2\"], cache[\"A2\"]\n",
        "    dZ2 = A2 - Y.T\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1 = np.dot(parameters[\"W2\"].T, dZ2)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = (1/m) * np.dot(dZ1, X)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qiHdcHAj6Jfr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Update parameters\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tWUfP3xa6NUq"
      },
      "outputs": [],
      "source": [
        "# Compute the loss\n",
        "def compute_loss(A2, Y):\n",
        "    m = Y.shape[0]\n",
        "    log_probs = np.multiply(np.log(A2.T), Y) + np.multiply((1 - Y), np.log(1 - A2.T))\n",
        "    loss = - np.sum(log_probs) / m\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6roc0JL06PLr"
      },
      "outputs": [],
      "source": [
        "# Predict function\n",
        "def predict(X, parameters):\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = np.argmax(A2, axis=0)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KtngPanH6Q_1"
      },
      "outputs": [],
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, hidden_size, epochs, learning_rate):\n",
        "    input_size = X_train.shape[1]\n",
        "    output_size = Y_train.shape[1]\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(epochs):\n",
        "        # Forward propagation\n",
        "        A2, cache = forward_propagation(X_train, parameters)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_loss(A2, Y_train)\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = backward_propagation(X_train, Y_train, cache, parameters)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the loss every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(\"Loss after iteration %i: %f\" % (i, loss))\n",
        "\n",
        "    # Predictions on training set\n",
        "    predictions_train = predict(X_train, parameters)\n",
        "    accuracy_train = np.mean(predictions_train == np.argmax(Y_train, axis=1)) * 100\n",
        "\n",
        "    # Predictions on test set\n",
        "    predictions_test = predict(X_test, parameters)\n",
        "    accuracy_test = np.mean(predictions_test == np.argmax(Y_test, axis=1)) * 100\n",
        "\n",
        "    print(\"Train accuracy: {:.2f}%\".format(accuracy_train))\n",
        "    print(\"Test accuracy: {:.2f}%\".format(accuracy_test))\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_j1UNI_H6WSQ"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "hidden_size = 64\n",
        "epochs = 1000\n",
        "learning_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuCA9RMj6ZA4",
        "outputId": "17772008-a663-4184-9957-30bd7304349c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 3.251248\n",
            "Loss after iteration 100: 1.738528\n",
            "Loss after iteration 200: 0.903747\n",
            "Loss after iteration 300: 0.705195\n",
            "Loss after iteration 400: 0.620553\n",
            "Loss after iteration 500: 0.572867\n",
            "Loss after iteration 600: 0.540771\n",
            "Loss after iteration 700: 0.516571\n",
            "Loss after iteration 800: 0.496885\n",
            "Loss after iteration 900: 0.479992\n",
            "Train accuracy: 92.15%\n",
            "Test accuracy: 91.91%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "parameters = model(X_train, y_train_encoded, X_test, y_test_encoded, hidden_size, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMY61fX66a4R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
